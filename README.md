# Twitter Sentiment Analysis 🌟

![Twitter-sentiment-analysis](https://github.com/user-attachments/assets/5d7181e8-2e8e-4289-ad36-5a6b6e27d233)

## Project Overview 🚀

This project focuses on performing sentiment analysis on Twitter data using various machine-learning techniques. We utilize the [Sentiment140 dataset](https://www.kaggle.com/kazanova/sentiment140) to classify tweets as positive or negative based on the sentiment expressed. 

## Table of Contents 📚

- [Features](#features)
- [Technologies Used](#technologies-used)
- [Getting Started](#getting-started)
- [Data Preprocessing](#data-preprocessing)
- [Modeling](#modeling)
- [Results](#results)
- [Significance of Models](#significance-of-models)
- [Future Improvements](#future-improvements)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## Features 🔍

- **Data Cleaning**: Comprehensive text preprocessing to enhance data quality.
- **Multiple Algorithms**: Implemented Logistic Regression, Multinomial Naive Bayes, and SVM classifiers for sentiment prediction.
- **Performance Evaluation**: Analyzed model performance using accuracy, precision, recall, and F1-score metrics.

## Technologies Used 💻

- **Programming Language**: Python
- **Libraries**: NLTK, Scikit-learn, Pandas, NumPy, Matplotlib
- **Dataset**: [Sentiment140](https://www.kaggle.com/kazanova/sentiment140)

## Getting Started 🛠️

To get started with this project, follow these steps:

### 1. Importing Dataset and Necessary Libraries:
NLTK, Scikit-learn, Pandas, NumPy, Matplotlib,Shutil,Logistic Regression,Multinomial NB, 


### 2. Data Preprocessing 🔄: 
  The preprocessing pipeline includes:
  Removing unwanted characters and links 🧹
  Converting text to lowercase 🔤
  Removing stop words 🚫
  Stemming the words for consistency 🌱

### 3. Modeling 🧠:

We implemented three machine learning models for sentiment analysis:

#### Logistic Regression:

A simple yet effective model for binary classification problems.
It assumes a linear relationship between the input features and the output, making it easy to interpret.

#### Multinomial Naive Bayes (MNB):

It is particularly effective for text classification tasks.
Based on Bayes' theorem, it assumes that features are independent given the class label, which simplifies calculations.
It works well with word frequencies, making it suitable for our dataset.

### 4. Results 📊
The models achieved promising accuracy in predicting sentiments:

Logistic Regression: 77.32%
Multinomial Naive Bayes: 76.27%

### 5.Significance of Models 📈
Utilizing these two models allows us to:

Compare Performance: Assessing the strengths and weaknesses of each algorithm helps identify the best approach for our specific dataset.

Robust Analysis: Different models may capture different aspects of the data, leading to a more comprehensive understanding of sentiment.

Diverse Strategies: Leveraging varied algorithms can improve overall accuracy and generalization to unseen data.

### 6.Future Improvements 🌱
Explore advanced NLP techniques such as word embeddings (e.g., Word2Vec, GloVe).
Implement deep learning models (e.g., LSTM) for better performance.
Enhance preprocessing steps by considering emojis and slang commonly used on Twitter.

### 7.Acknowledgments 🙏
Thanks to the creators of the Sentiment140 dataset.
Inspiration from geeks for geeks sentimental analysis youtube video and python for programers by Dietel book.
